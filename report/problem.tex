\section{Постановка задачи}
Была поставлена задача провести эксперимент с выявлением лучших качеств
алгоритмов сортировки. Алгоритм должен быть эффективным с точки зрения
потребления ресурсов процессора и оперативной памяти. Благодаря этому
программное обеспечение, требующее отсортированных массивов данных, будет 
работать быстрее и сможет обрабатывать больший объем информации.\par
Однако, невозможно выделить самый лучший алгоритм сортировки. Их эффективность
и скорость работы сильно зависят от структуры исходных данных. В связи с этим,
необходимо провести эксперимент с разными массивами данных, чтобы установить зависимость
между структурой информации и скоростью алгоритма сортировки.\par
Следующие алгоритмы были отобраны для участия в эксперименте как наиболее распространенные:
\begin{enumerate}
    \item Пузырьковая сортировка
    \item Шейкерная (двунаправленная) сортировка
    \item Сортировка вставками
    \item Сортировка Шелла
    \item Сортировка выбором
    \item Сортировка слиянием
\end{enumerate}
Ниже будут рассмотртрена теоретическая оценка скорости каждого из алгоритмов.

\subsection{Пузырьковая сортировка}
Попарное сравнение элементов - наиболее очевидное решение проблемы сортировки.
Если предположить, что в массиве содержится $N$ элементов и хотя бы один из них
занимает свое место в результате однократного просмотра значений, то алгоритм может
совершить не более $N$ проходов (Все $N$ понадобятся, если алгоритм изначально
отсортирван в обратном порядке). Каждый проход включает в себя $N$ шагов. Отсюда
общее время работы - \textbf{$O(N^2)$}. Так как сортировка относится к классу внутренних
и не использует дополнительную память, ее затраты составляют \textbf{$O(1)$}.\cite{Stephens}

\subsection{Шейкерная (двунаправленная) сортировка}
Анализируя метод пузырьковой сортировки, можно отметить два обстоятельства.Во-первых, если при 
движении по части массива перестановки не происходят, то эта часть массива уже отсортирована и, 
следовательно, её можно исключить из рассмотрения. Во-вторых, при движении от конца массива 
к началу минимальный элемент «всплывает» на первую позицию, а максимальный элемент сдвигается 
только на одну позицию вправо\cite{cocktailsort:wiki}.\par
Сложность алгоритма имеет порядок $O(N^2)$ для худшего и среднего случая. Но она приближается
к $O(N)$ в том случае, если данные уже частично упорядочены. Например, если позиция каждого элемента
отличается не более чем на $k \geqslant 1$ от верной позиции, то алгоритм отработает за $O(kN)$. \par
Шейкерная сортировка и другие улучшения подробно рассматриваются в книге \citetitle{Knuth3} Дональда Кнута. 
В частности автор приходит к следующим выводам:
\begin{displayquote}
\textit{
  But none of these refinements leads to an algorithm better than straight insertion [that is, 
  insertion sort]; and we already know that straight insertion isn't suitable for large N. 
  [...] In short, the bubble sort seems to have nothing to recommend it, except a catchy name 
  and the fact that it leads to some interesting theoretical problems.
}\break
\hfill D. E. Knuth
\end{displayquote}
\begin{displayquote}
\textit{
    Но ни одно из этих улучшений не приводит к лучшему алгоритму, чем прямые вставки [да, сортировка вставками], 
    и мы уже знаем, что сортировки вставками не подходят для больших N. [...] В кратце, кажется, что
    пузырек не за что рекомендовать, за исключением броского названия и факта, что он приводит к некоторым
    интересным теоретическим проблемам.
}\break
\hfill Д. Э. Кнут
\end{displayquote}

\subsection{Сортировка вставками}
Наихудшим случаем является массив, отсортированный в порядке, обратном нужному. При этом 
каждый новый элемент сравнивается со всеми в отсортированной последовательности. Это означает, 
что все внутренние циклы состоят из $j$ итераций, то есть $t_{j}=j$ для всех $j$. Тогда время 
работы алгоритма составит\cite{inssort:wiki}:
$$T(n)=c_{1}n+c_{2}(n-1)+c_{3}(n-1)+c_{4}\sum _{{j=2}}^{n}j+c_{5}\sum _{{j=2}}^{n}(j-1)+c_{6}\sum _{{j=2}}^{n}(j-1)+c_{7}(n-1)$$
$$T(n)=c_{1}n+c_{2}(n-1)+c_{3}(n-1)+c_{4}({\frac {n(n+1)}{2}}-1)+c_{5}{\frac {n(n-1)}{2}}+c_{6}{\frac {n(n-1)}{2}}+c_{7}(n-1)=O(n^{2})$$
Время работы является квадратичной функцией от размера входных данных.\par
Для анализа среднего случая нужно посчитать среднее число сравнений, необходимых для определения 
положения очередного элемента. При добавлении нового элемента потребуется, как минимум, одно сравнение, 
даже если этот элемент оказался в правильной позиции. $i$-й добавляемый элемент может занимать одно из 
$i+1$ положений. Предполагая случайные входные данные, новый элемент равновероятно может оказаться в 
любой позиции. Среднее число сравнений для вставки $i$-го элемента:
$$T_{i}={\frac {1}{i+1}}(\sum _{p=1}^{i}p+i)={\frac {1}{i+1}}({\frac {i(i+1)}{2}}+i)={\frac {i}{2}}+1-{\frac {1}{i+1}}$$
Для оценки среднего времени работы для n элементов нужно просуммировать\cite{inssort:wiki}:
$$T(n)=\sum _{{i=1}}^{{n-1}}T_{i}=\sum _{{i=1}}^{{n-1}}({\frac {i}{2}}+1-{\frac {1}{i+1}})=\sum _{{i=1}}^{{n-1}}{\frac {i}{2}}+\sum _{{i=1}}^{{n-1}}1-\sum _{{i=1}}^{{n-1}}{\frac {1}{i+1}})$$
$$T(n)\approx {\frac {n^{2}-n}{4}}+(n-1)-(ln(n)-1)=O(n^{2})$$
Временная сложность алгоритма — $O(N^{2})$. Однако, из-за константных множителей и членов более 
низкого порядка алгоритм с более высоким порядком роста может выполняться для небольших входных 
данных быстрее, чем алгоритм с более низким порядком роста.

\subsection{Сортировка Шелла}
Для алгоритма сортировки, который каждый раз перемещает запись только на одну позицию, среднее время
выполнения будет в лучшем случае пропорционально $N^2$, потому что в процессе сортировки каждый элемент
должен пройти в среднем через $\frac{1}{3}N$ позиций. Поэтому желательно получить метод, существенно 
превосходящий по скорости метод простых вставок с помощью механизма, позволяющего элементам перемещаться
большими скачками, а не короткими шажками\cite{Knuth3}.\par
В 1959 году Шелл предложил такой метод. Анализ этого алгоритма – сложная математическая задача, у 
которой до сих пор нет полного решения\cite{sortagain:website}. В настоящий момент неизвестно, какая последовательность 
расстояний даёт наилучший результат, но известно, что расстояния не должны быть кратными друг другу.\par 
Кнут предлагает в качестве последовательно уменьшающихся расстояний использовать одну из следующих 
последовательностей (приведены в обратном порядке): $1,4,13,40,…$, где $h_{i-1}=3*h_{i}+1$ или 
$1,3,7,15,31,…$, где $h_{i-1}=2*h_{i}+1$. В последнем случае математическое исследование показывает, 
что при сортировке $N$ элементов алгоритмом Шелла затраты пропорциональны $N^{1,2}$.

\subsection{Сортировка выбором}
Анализ сортировки выбором не сложен, так как сложность не зависит от данных. Поиск минимального элемента
требует просмотра $N$ элементов (это $N-1$ сравнений). Поиск следующего минимального требует просмотра $N-1$ 
элементов и так далее. Таким образом, сложность алгоритма можно оценить с помощью следующего выражения:
$$(N-1)+(N-2)+...+2+1 = \frac{N(N-1)}{2} = O(N^2)$$

\subsection{Сортировка слиянием}
При сортировке $N$ элементов сортировка слиянием имеет в худшем и среднем случае сложность $O(N\log{N})$.
Формула зависимости времени выполнения алгоритма от количества данных следует из его определения\cite{mergesort:wiki}:
$$T(N) = 2T(\frac{N}{2}) + N$$
При этом сортировка слиянием относится к классу внешних и требует дополнительной памяти. Оптимальная реализация
требует однократного выделения памяти, затраты которой равны $O(N)$.